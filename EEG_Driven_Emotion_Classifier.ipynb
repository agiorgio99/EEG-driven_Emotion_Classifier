{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAMlS5gNAwjf"
      },
      "source": [
        "# **Installations**\n",
        "\n",
        "This section outlines the essential setup steps for working with **Llama-3.1-8B-Instruct** model. It includes installing necessary packages, authenticating with Hugging Face, and downloading model weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Install Required Packages**\n",
        "   - `bitsandbytes` for efficient 8-bit optimizations.\n",
        "   - `transformers`, `accelerate`, and `peft` for model loading, training acceleration, and parameter-efficient fine-tuning.\n",
        "   - `python-dotenv` for managing environment variables.\n",
        "   - `einops`, `scikit-learn`, and `scipy` for tensor operations, machine learning utilities, and scientific computing.\n",
        "   - `matplotlib` for data visualization.\n",
        "   - `tabulate` for neatly formatted textual tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-0g0jFnUjld",
        "outputId": "bfadfa4d-7af6-41b7-8330-7fd06d8f4ded"
      },
      "outputs": [],
      "source": [
        "%pip install -U bitsandbytes\n",
        "%pip install -U transformers accelerate peft\n",
        "%pip install python-dotenv\n",
        "%pip install einops scikit-learn scipy\n",
        "%pip install matplotlib\n",
        "%pip install tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. **Hugging Face Authentication**  \n",
        "   Authenticate with **Hugging Face** using an access token. This allows access to Llama Model Weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hugging Face login\n",
        "hf_token = \"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\"\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. **Download the Llama Model Weights**  \n",
        "   The Hugging Face CLI is used to download the **Llama-3.1-8B-Instruct** model weights into a local directory while excluding unnecessary paths (`original/*`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Llama model weights\n",
        "!huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --local-dir Llama-3.1-8B-Instruct --exclude \"original/*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Preprocessing**\n",
        "\n",
        "This section defines the **DataProcessor** class, which preprocesses EEG data through normalization, segmentation, and quantization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Loading EEG Data**  \n",
        "   `load_subject_data` reads `.dat` EEG files and extracts the data and labels.\n",
        "\n",
        "    <div style=\"text-align: left;\">\n",
        "     <img src=\"Images/data_preprocessed_python_description.png\" width=\"70%\">\n",
        "    </div>\n",
        "\n",
        "    In addition to the 32 EEG channels, the dataset includes 8 channels that contain the following physiological signals:\n",
        "\n",
        "    - **EOG (Electrooculography):** Horizontal (hEOG₁, hEOG₂) and Vertical (vEOG₁, vEOG₂).\n",
        "    - **EMG (Electromyography):** Zygomaticus Major EMG (zEMG₁, zEMG₂) and Trapezius EMG (tEMG₁, tEMG₂).\n",
        "    - **GSR (Galvanic Skin Response):** Values converted from Twente to Geneva format (ohms).\n",
        "    - **Respiration Belt, Plethysmograph, and Temperature Sensors**.\n",
        "\n",
        "    These 8 channels are exclued from the input data.\n",
        "\n",
        "    The following preprocessing steps were applied to the EEG data:\n",
        "\n",
        "    1. Downsampled to **128 Hz**.\n",
        "    2. EOG artifacts removed as described in **[1]**.\n",
        "    3. Bandpass frequency filter applied (**4.0 - 45.0 Hz**).\n",
        "    4. Averaged to a **common reference**.\n",
        "    5. EEG channels reordered to match the **Geneva order**.\n",
        "    6. Segmented into **60-second trials**, removing the **3-second pre-trial baseline**.\n",
        "    7. Trials reordered from **presentation order** to **video (Experiment_id) order**.\n",
        "\n",
        "\n",
        "2. **Z-score Normalization**  \n",
        "   `zscore_normalize` standardizes EEG signals across trials and channels, ensuring zero mean and unit variance. The transformation is given by:\n",
        "   \n",
        "   <div style=\"text-align: left;\">\n",
        "    <img src=\"Images/zscore.png\" width=\"50%\">\n",
        "   </div>\n",
        "\n",
        "   This normalization helps to make EEG signals comparable across different subjects and trials.\n",
        "\n",
        "3. **Quantization Bins and Labels**  \n",
        "   `analyze_distribution` flattens the EEG data, computes percentile-based bins (5th-95th percentiles), and assigns labels. Labels can be **binary** (e.g., `000`, `001`, `010`) or **symbolic** (`A`, `B`, `C`).\n",
        "\n",
        "   <div style=\"text-align: left;\">\n",
        "    <img src=\"Images/EEG Data Quantization Bins.png\" width=\"50%\">\n",
        "   </div>\n",
        "\n",
        "\n",
        "4. **EEG Segmentation**  \n",
        "   `segment_eeg_data` splits EEG trials into overlapping windows of size `window_size`, with step size determined by `overlap`.\n",
        "    \n",
        "    <div style=\"text-align: left;\">\n",
        "     <img src=\"Images/EEG Segmentation With Overlapping Windows.png\" width=\"50%\">\n",
        "    </div>\n",
        "\n",
        "5. **Signal Quantization**  \n",
        "   `quantize_signal` maps EEG values into quantization bins, producing a compact, space-separated sequence.\n",
        "\n",
        "    <div style=\"text-align: left;\">\n",
        "     <img src=\"Images/EEG Quantization With Binary Encoding.png\" width=\"50%\">\n",
        "    </div>\n",
        "\n",
        "6. **Full Subject Preprocessing**  \n",
        "   `preprocess_subject` applies the pipeline:\n",
        "   - Extracts the first 32 EEG channels.\n",
        "   - Normalizes EEG data (z-score).\n",
        "   - Normalizes valence/arousal labels to \\([0,1]\\).\n",
        "   - Computes quantization bins.\n",
        "   - Segments and quantizes each trial.\n",
        "   - Replicates labels for each segment.\n",
        "\n",
        "7. **Processing the DEAP Dataset**  \n",
        "   `preprocess_deap_data` iterates over `.dat` files, applies preprocessing, and saves sequences and labels as NumPy arrays.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility class for EEG preprocessing and quantization\n",
        "class DataProcessor:\n",
        "\n",
        "    def __init__(self, preprocessed_output_dir, num_bins, bin_encoding, window_size, overlap):\n",
        "        \n",
        "        \"\"\"\n",
        "        Handles subject-level EEG loading, normalization, segmentation, and quantization.\n",
        "\n",
        "        Args:\n",
        "            preprocessed_output_dir (str): Directory to save preprocessed data.\n",
        "            num_bins (int): Number of bins for quantization.\n",
        "            bin_encoding (str): Encoding method ('binary' or 'symbolic').\n",
        "            window_size (int): Number of samples per segment.\n",
        "            overlap (float): Fraction of overlap between windows.\n",
        "        \"\"\"\n",
        "        self.preprocessed_output_dir = preprocessed_output_dir\n",
        "        self.num_bins = num_bins\n",
        "        self.bin_encoding = bin_encoding\n",
        "        self.window_size = window_size\n",
        "        self.overlap = overlap\n",
        "        self.bins = None\n",
        "        self.labels = None\n",
        "\n",
        "\n",
        "\n",
        "    def load_subject_data(self, file_path):\n",
        "\n",
        "        \"\"\"\n",
        "        Load EEG data and corresponding labels from a .dat file.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the .dat file.\n",
        "\n",
        "        Returns:\n",
        "            tuple: EEG data and labels as numpy arrays.\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"Loading data from {file_path}...\")\n",
        "        with open(file_path, 'rb') as f:\n",
        "            subject_data = pickle.load(f, encoding='latin1')\n",
        "            print(\"Data loaded successfully.\")\n",
        "            return subject_data['data'], subject_data['labels']\n",
        "\n",
        "\n",
        "\n",
        "    def zscore_normalize(self, eeg_data):\n",
        "        \n",
        "        \"\"\"\n",
        "        Perform z-score normalization across channels and time for each subject’s entire data.\n",
        "        eeg_data shape: (num_trials, num_eeg_channels, time) ->  (40, 32, 8064).\n",
        "\n",
        "        Args:\n",
        "            eeg_data (np.ndarray): EEG data to be normalized.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Z-score normalized EEG data.\n",
        "        \"\"\"\n",
        "\n",
        "        # shape: (trial, channel, time)\n",
        "        mean_vals = np.mean(eeg_data, axis=(0,2), keepdims=True)\n",
        "        std_vals = np.std(eeg_data, axis=(0,2), keepdims=True)\n",
        "        eeg_data = (eeg_data - mean_vals) / (std_vals + 1e-7)\n",
        "        return eeg_data    \n",
        "    \n",
        "\n",
        "\n",
        "    def analyze_distribution(self, eeg_data):\n",
        "        \n",
        "        \"\"\"\n",
        "        Analyze EEG amplitude distribution and define quantization bins.\n",
        "\n",
        "        Args:\n",
        "            eeg_data (np.ndarray): EEG data of shape (num_trials, 32, time_steps).\n",
        "\n",
        "        Returns:\n",
        "            Compute quantization bins, and updates self.bins and self.labels according to them.\n",
        "        \"\"\"\n",
        "        \n",
        "        flattened_data = eeg_data.flatten()\n",
        "        # Compute percentiles from 5th to 95th to avoid outliers\n",
        "        percentiles = np.linspace(5, 95, self.num_bins + 1)\n",
        "        self.bins = np.percentile(flattened_data, percentiles)\n",
        "\n",
        "        # Assign labels (binary or symbolic)\n",
        "        if self.bin_encoding == \"binary\":\n",
        "            # e.g. 3-bit if num_bins=8 => '000', '001', '010', ...\n",
        "            self.labels = [\n",
        "                format(i, f'0{len(bin(self.num_bins - 1)[2:])}b')\n",
        "                for i in range(self.num_bins)\n",
        "            ]\n",
        "        else:\n",
        "            # e.g. A, B, C, ...\n",
        "            self.labels = [chr(65 + i) for i in range(self.num_bins)]\n",
        "\n",
        "        print(f\"Quantization Bins: {self.bins}\")\n",
        "        print(f\"Assigned Labels: {self.labels}\")\n",
        "\n",
        "\n",
        "    \n",
        "    def segment_eeg_data(self, eeg_data):\n",
        "\n",
        "        \"\"\"\n",
        "        Segment EEG data into overlapping windows.\n",
        "\n",
        "        Args:\n",
        "            eeg_data (np.ndarray): EEG data of shape (32, 8064).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Segmented EEG data of shape (num_segments, 32, window_size).\n",
        "        \"\"\"\n",
        "\n",
        "        step = int(self.window_size * (1 - self.overlap))\n",
        "        num_windows = (eeg_data.shape[1] - self.window_size) // step + 1\n",
        "        print(f\"Segmenting EEG data into {num_windows} windows...\")\n",
        "        segments = [\n",
        "            eeg_data[:, i * step:i * step + self.window_size]\n",
        "            for i in range(num_windows)\n",
        "        ]\n",
        "        print(\"Segmentation complete.\")\n",
        "        return np.stack(segments, axis=0)\n",
        "\n",
        "    \n",
        "\n",
        "    def quantize_signal(self, signal):\n",
        "\n",
        "        \"\"\"\n",
        "        Convert an EEG signal into a space-separated quantized representation.\n",
        "\n",
        "        Args:\n",
        "            signal (np.ndarray): Single EEG trial of shape (32, window_size).\n",
        "\n",
        "        Returns:\n",
        "            str: Space-separated quantized representation.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.bins is None:\n",
        "            raise ValueError(\"Bins not initialized. Run analyze_distribution() first.\")\n",
        "        \n",
        "        # Flatten the 32 channels for that segment\n",
        "        flat = signal.flatten()\n",
        "        quantized_indices = np.digitize(flat, self.bins, right=False) - 1\n",
        "        quantized_indices = np.clip(quantized_indices, 0, len(self.labels) - 1)\n",
        "        return ' '.join(self.labels[i] for i in quantized_indices)\n",
        "    \n",
        "    \n",
        "    \n",
        "    def preprocess_subject(self, subject_file):\n",
        "\n",
        "        \"\"\"\n",
        "        Preprocess a single subject's EEG data: z-score, segment, quantize, normalize labels.\n",
        "\n",
        "        Args:\n",
        "            subject_file (str): Path to the subject's .dat file.\n",
        "\n",
        "        Returns:\n",
        "            tuple: z-scored, segmented, quantized EEG data and normalized labels.\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"Preprocessing data for {subject_file}...\")\n",
        "        eeg_data, labels = self.load_subject_data(subject_file)\n",
        "    \n",
        "        # eeg_data => (40, 40, 8064) video/trial x channel x data, \n",
        "        # labels => (40, 4) video/trial x label (valence, arousal, dominance, liking) \n",
        "\n",
        "        # We only need the first 32 channels, \n",
        "        # because the remaining 8 are other physiological data, so:\n",
        "\n",
        "        # 1) Keep only the first 32 channels and time dimension\n",
        "        eeg_data = eeg_data[:, :32, :]  \n",
        "\n",
        "        # 2) Z-score per subject\n",
        "        eeg_data = self.zscore_normalize(eeg_data)\n",
        "\n",
        "        # 3) Valence & arousal only => columns 0 & 1, normalizing from [1,9] to [0,1]\n",
        "        labels = labels[:, :2]  \n",
        "        labels = (labels - 1) / 8\n",
        "\n",
        "        # 4) Compute quantization bins based on the entire subject’s EEG distribution\n",
        "        #    (Now that it’s z-scored).\n",
        "        self.analyze_distribution(eeg_data)\n",
        "\n",
        "        all_sequences = []\n",
        "        all_labels = []\n",
        "\n",
        "        for trial_idx, trial_data in enumerate(eeg_data):\n",
        "            segments = self.segment_eeg_data(trial_data)\n",
        "            # Quantize each segment\n",
        "            quantized_segments = [self.quantize_signal(seg) for seg in segments]\n",
        "\n",
        "            all_sequences.extend(quantized_segments)\n",
        "\n",
        "            # Duplicate this trial's valence/arousal label for each segment\n",
        "            trial_labels = np.tile(labels[trial_idx], (len(quantized_segments), 1))\n",
        "            all_labels.append(trial_labels)\n",
        "\n",
        "        sequences = np.array(all_sequences, dtype=object)\n",
        "        labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "        # For debugging\n",
        "        print(f\"Preprocessed data dimensions => Sequences: {sequences.shape}, Labels: {labels.shape}\")\n",
        "        return sequences, labels\n",
        "\n",
        "\n",
        "\n",
        "    def save_hyperparameters(self):\n",
        "        hyperparams = {\n",
        "            \"num_bins\": self.num_bins,\n",
        "            \"bin_encoding\": self.bin_encoding,\n",
        "            \"window_size\": self.window_size,\n",
        "            \"overlap\": self.overlap\n",
        "        }\n",
        "        hyperparams_path = os.path.join(self.preprocessed_output_dir, \"preprocessing_hyperparameters.json\")\n",
        "        with open(hyperparams_path, 'w') as f:\n",
        "            json.dump(hyperparams, f, indent=4)\n",
        "        print(f\"Saved preprocessing hyperparameters to {hyperparams_path}\")\n",
        "\n",
        "        \n",
        "\n",
        "    def preprocess_deap_data(self, data_path):\n",
        "\n",
        "        \"\"\"\n",
        "        Preprocess all subjects' data in the DEAP dataset.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the folder containing .dat files.\n",
        "\n",
        "        Returns:\n",
        "            Saves sequences with shape (num_segments, 32, window_size) and labels with shape (num_segments, 2).\n",
        "        \"\"\"\n",
        "\n",
        "        os.makedirs(self.preprocessed_output_dir, exist_ok=True)\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        for subject_file in os.listdir(data_path):\n",
        "            if subject_file.endswith(\".dat\"):\n",
        "                print(f\"Processing {subject_file}...\")\n",
        "                subject_path = os.path.join(data_path, subject_file)\n",
        "                sequences, labels = self.preprocess_subject(subject_path)\n",
        "\n",
        "                # Overwrite existing files without checking\n",
        "                np.save(os.path.join(self.preprocessed_output_dir, f\"{subject_file}_sequences.npy\"), sequences)\n",
        "                np.save(os.path.join(self.preprocessed_output_dir, f\"{subject_file}_labels.npy\"), labels)\n",
        "                print(f\"Saved preprocessed data for {subject_file}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess data\n",
        "\n",
        "num_bins=8\n",
        "bin_encoding=\"binary\"\n",
        "window_size = 1024\n",
        "overlap = 0.1\n",
        "\n",
        "processor = DataProcessor(preprocessed_output_dir, num_bins, bin_encoding, window_size, overlap)\n",
        "processor.preprocess_deap_data(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Imports and Directories**\n",
        "\n",
        "This section configures GPU settings, ensures reproducibility, and sets up paths for dataset preprocessing and model usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of GPUs: 4\n",
            "GPU 0: NVIDIA RTX A6000\n",
            "GPU 1: NVIDIA RTX A6000\n",
            "GPU 2: NVIDIA RTX A6000\n",
            "GPU 3: NVIDIA RTX A6000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "num_gpus = torch.cuda.device_count()  # Get the number of available GPUs\n",
        "print(f\"Number of GPUs: {num_gpus}\")\n",
        "\n",
        "for i in range(num_gpus):\n",
        "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**GPU Detection & Selection**  \n",
        "   - Identifies available GPUs and selects a specific one for computations.  \n",
        "   - Retrieves and displays GPU specifications, including memory, multiprocessors, and compute capability.\n",
        "   - Sets the selected GPU as the active device. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:3\n",
            "Using GPU: NVIDIA RTX A6000\n",
            "Device Count: 4\n",
            "Current Device ID: 3\n",
            "CUDA is Available: True\n",
            "\n",
            " GPU Specifications:\n",
            "   - Name: NVIDIA RTX A6000\n",
            "   - Total Memory: 51.03 GB\n",
            "   - Multiprocessors: 84\n",
            "   - Compute Capability: 8.6\n",
            "   - Max Threads per Multiprocessor: 1536\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "gpu_id = 2\n",
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "if gpu_id >= num_gpus:\n",
        "\traise ValueError(f\"Invalid GPU ID {gpu_id}. Only {num_gpus} GPUs are available.\")\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
        "\n",
        "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.set_device(gpu_id)\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Print CUDA device information\n",
        "print(\"Using GPU:\", torch.cuda.get_device_name(gpu_id))\n",
        "print(\"Device Count:\", torch.cuda.device_count())\n",
        "print(\"Current Device ID:\", torch.cuda.current_device())\n",
        "print(\"CUDA is Available:\", torch.cuda.is_available())\n",
        "\n",
        "# Get device properties\n",
        "device_props = torch.cuda.get_device_properties(gpu_id)\n",
        "print(\"\\n GPU Specifications:\")\n",
        "print(f\"   - Name: {device_props.name}\")\n",
        "print(f\"   - Total Memory: {device_props.total_memory / 1e9:.2f} GB\")\n",
        "print(f\"   - Multiprocessors: {device_props.multi_processor_count}\")\n",
        "print(f\"   - Compute Capability: {device_props.major}.{device_props.minor}\")\n",
        "print(f\"   - Max Threads per Multiprocessor: {device_props.max_threads_per_multi_processor}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "if gpu_id >= num_gpus:\n",
        "\traise ValueError(f\"Invalid GPU ID {gpu_id}. Only {num_gpus} GPUs are available.\")\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
        "\n",
        "device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.set_device(gpu_id)\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Print CUDA device information\n",
        "print(\"Using GPU:\", torch.cuda.get_device_name(gpu_id))\n",
        "print(\"Device Count:\", torch.cuda.device_count())\n",
        "print(\"Current Device ID:\", torch.cuda.current_device())\n",
        "print(\"CUDA is Available:\", torch.cuda.is_available())\n",
        "\n",
        "# Get device properties\n",
        "device_props = torch.cuda.get_device_properties(gpu_id)\n",
        "print(\"\\n GPU Specifications:\")\n",
        "print(f\"   - Name: {device_props.name}\")\n",
        "print(f\"   - Total Memory: {device_props.total_memory / 1e9:.2f} GB\")\n",
        "print(f\"   - Multiprocessors: {device_props.multi_processor_count}\")\n",
        "print(f\"   - Compute Capability: {device_props.major}.{device_props.minor}\")\n",
        "print(f\"   - Max Threads per Multiprocessor: {device_props.max_threads_per_multi_processor}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R-GLNcEEntUQ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reproducibility**  \n",
        "A fixed seed is set to maintain consistency in results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f237c1e39b0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set reproducibility seed\n",
        "seed = 42\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Directories**  \n",
        "Modify the paths to match project folder configuration. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l_RgLiD2SOwe"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "project_path = \"./\"\n",
        "data_path = \"../../data/DEAP_Dataset/data_preprocessed_python\"\n",
        "preprocessed_output_dir = os.path.join(project_path, \"DEAP_preprocessed\")\n",
        "model_path = \"Llama-3.1-8B-Instruct\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC0FxjL8BEig"
      },
      "source": [
        "# **Dataset**\n",
        "\n",
        "This section describes the structure of the **DEAP dataset**, the preprocessing steps applied to EEG data, and how the data is formatted for machine learning. The EEG signals are transformed into quantized text sequences, while valence and arousal labels are discretized into categorical classes for affective computing tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**EEG Data Representation**\n",
        "\n",
        "The DEAP dataset consists of EEG signals recorded at $128$ Hz across $32$ channels for each trial. Each participant undergoes $40$ trials, resulting in a raw EEG data matrix:\n",
        "\n",
        "$$\n",
        "\\mathbf{X}_{\\text{raw}} \\in \\mathbb{R}^{n_{\\text{trials}} \\times 32 \\times 8064}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $n_{\\text{trials}}$ is the total number of trials across all participants,\n",
        "- $32$ represents the EEG channels,\n",
        "- $8064 = 128 \\cdot 60 + 3$ accounts for the number of samples per trial (1-minute recording + 3-second baseline).\n",
        "\n",
        "Each trial is associated with an affective label vector:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} \\in \\mathbb{R}^{n_{\\text{trials}} \\times 4}\n",
        "$$\n",
        "\n",
        "where the four elements represent **valence, arousal, dominance, and liking**, rated on a scale from 1 to 9.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: left;\">\n",
        " <img src=\"Images/experiment_participant_setup.png\" width=\"50%\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "   <div style=\"text-align: left;\">\n",
        "    <img src=\"Images/video_examples.png\" width=\"50%\">\n",
        "   </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_all_preprocessed_subjects(preprocessed_output_dir, max_subjects=None):\n",
        "    \"\"\"\n",
        "    Reads each file ending with \"_sequences.npy\" in `preprocessed_output_dir`,\n",
        "    and finds the corresponding \"_labels.npy\" file.\n",
        "    \n",
        "    If `max_subjects` is None, load ALL available subject files.\n",
        "    Otherwise, load only the first `max_subjects` files (sorted alphabetically).\n",
        "\n",
        "    Returns:\n",
        "        all_sequences: (N,) array of quantized EEG text segments\n",
        "        all_labels: (N, 2) array of valence, arousal\n",
        "    \"\"\"\n",
        "    all_seq_files = sorted(\n",
        "        f for f in os.listdir(preprocessed_output_dir) if f.endswith(\"_sequences.npy\")\n",
        "    )\n",
        "\n",
        "    if max_subjects is not None:\n",
        "        all_seq_files = all_seq_files[:max_subjects]\n",
        "\n",
        "    all_sequences = []\n",
        "    all_labels = []\n",
        "\n",
        "    for seq_filename in all_seq_files:\n",
        "        seq_path = os.path.join(preprocessed_output_dir, seq_filename)\n",
        "        lab_path = seq_path.replace(\"_sequences.npy\", \"_labels.npy\")\n",
        "        \n",
        "        if not os.path.exists(lab_path):\n",
        "            print(f\"Warning: Labels file not found for {seq_filename}\")\n",
        "            continue\n",
        "        \n",
        "        subject_sequences = np.load(seq_path, allow_pickle=True)\n",
        "        subject_labels = np.load(lab_path, allow_pickle=True)\n",
        "\n",
        "        all_sequences.append(subject_sequences)\n",
        "        all_labels.append(subject_labels)\n",
        "\n",
        "    if len(all_sequences) == 0:\n",
        "        raise ValueError(\"No preprocessed subject files found in the directory.\")\n",
        "\n",
        "    all_sequences = np.concatenate(all_sequences, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    print(f\"Total loaded sequences: {all_sequences.shape}\")\n",
        "    print(f\"Total loaded labels: {all_labels.shape}\")\n",
        "    return all_sequences, all_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Preprocessed Data Format**\n",
        "\n",
        "The preprocessed dataset extracts EEG features and represents each trial as a sequence of text-encoded EEG segments. The dataset is stored as:\n",
        "\n",
        "- **EEG sequences:** $\\mathbf{S}$, stored in `_sequences.npy`\n",
        "- **Corresponding labels:** $\\mathbf{L}$, stored in `_labels.npy`\n",
        "\n",
        "After preprocessing, the dataset is structured as:\n",
        "\n",
        "$$\n",
        "\\mathbf{S} \\in \\mathbb{R}^{N}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{L} \\in [0,1]^{N \\times 2}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $N$ is the number of extracted EEG segments across all trials.\n",
        "- Each sequence in $\\mathbf{S}$ represents a quantized text-encoded EEG segment.\n",
        "- The label matrix $\\mathbf{L}$ contains valence and arousal scores normalized between 0 and 1.\n",
        "\n",
        "**Label Discretization**\n",
        "\n",
        "The continuous valence ($v$) and arousal ($a$) scores are discretized into two classes:\n",
        "\n",
        "$$\n",
        "C(x) =\n",
        "\\begin{cases}\n",
        "0, & x \\leq 0.56 \\\\\n",
        "1, & x > 0.56\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "This results in a new label matrix:\n",
        "\n",
        "$$\n",
        "\\mathbf{L'} \\in \\{0,1\\}^{N \\times 2}\n",
        "$$\n",
        "\n",
        "where each label is transformed into discrete categories for classification tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HpfGgLLOn4KF"
      },
      "outputs": [],
      "source": [
        "class DEAPDataset(Dataset):\n",
        "\n",
        "    def __init__(self, sequences, labels, debug=False):\n",
        "\n",
        "        \"\"\"\n",
        "        sequences: array/list of text strings (quantized EEG), one per segment\n",
        "        labels: shape [num_segments, 2] => valence, arousal\n",
        "        debug: print sample info for debugging\n",
        "        \"\"\"\n",
        "        \n",
        "        self.sequences = sequences\n",
        "        self.debug = debug\n",
        "\n",
        "        # Convert each [val, aro] from [0..1] to discrete {0,1,2}\n",
        "        discrete = []\n",
        "        for (v, a) in labels:\n",
        "            v_class = self._continuous_to_class(v)\n",
        "            a_class = self._continuous_to_class(a)\n",
        "            discrete.append([v_class, a_class])\n",
        "        self.labels = np.array(discrete, dtype=np.int64)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        \"\"\"\n",
        "        Return the raw text segment and label (no tokenization here).\n",
        "        \"\"\"\n",
        "\n",
        "        text_segment = self.sequences[idx]\n",
        "        label = self.labels[idx] # shape: (2,) => [val_class, aro_class]\n",
        "\n",
        "        if self.debug and idx < 1:\n",
        "            print(f\"Example sequence: {self.sequences[0]}\")\n",
        "            print(f\"Example label: {self.labels[0]}  (valence_class, arousal_class)\")\n",
        "        # Return the raw text and label as a tuple\n",
        "        return text_segment, label\n",
        "\n",
        "    def _continuous_to_class(self, value):\n",
        "        if value <= 5/9:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Dataset Construction**\n",
        "\n",
        "A PyTorch `Dataset` class organizes the sequences and labels, enabling:\n",
        "- Efficient access to EEG text segments and labels.\n",
        "- On-the-fly retrieval of raw EEG sequences and corresponding valence/arousal categories.\n",
        "- Debugging mode for inspecting sample data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total loaded sequences: (10240,)\n",
            "Total loaded labels: (10240, 2)\n"
          ]
        }
      ],
      "source": [
        "# Decide how many subject files to load\n",
        "# e.g. set `max_subjects=2` to load only 2 subject files, or None for all\n",
        "max_subjects = None  # or None\n",
        "\n",
        "# Load preprocessed (optionally limited) subject files\n",
        "sequences, labels = load_all_preprocessed_subjects(\n",
        "    preprocessed_output_dir,\n",
        "    max_subjects=max_subjects\n",
        ")\n",
        "dataset = DEAPDataset(sequences, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **DataLoader**\n",
        "\n",
        "This section defines the **collate function** used in the `DataLoader`. The function **tokenizes text segments in batches**, ensuring consistent input formatting for the LLaMA model. It also processes labels and moves all tensors to the specified device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dynamic_tokenize_collate_fn(tokenizer, max_length, device, debug=False):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns a function that can be used as collate_fn in the PyTorch DataLoader.\n",
        "    The returned function tokenizes the raw text segments in batch.\n",
        "    \"\"\"\n",
        "\n",
        "    def collate_fn(batch):\n",
        "\n",
        "        \"\"\"\n",
        "        batch: list of (text_segment, label) tuples\n",
        "        \"\"\"\n",
        "        \n",
        "        # Separate text and labels\n",
        "        text_segments = [item[0] for item in batch]\n",
        "        labels = [item[1] for item in batch]  # shape: [val_class, aro_class]\n",
        "\n",
        "        if debug and len(text_segments) > 0:\n",
        "            print(f\"\\n[CollateFn] Example text: {text_segments[0]}\")\n",
        "\n",
        "        # Tokenize in batch\n",
        "        encoded = tokenizer(\n",
        "            text_segments,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Extract tensors\n",
        "        input_ids = encoded[\"input_ids\"]\n",
        "        attention_mask = encoded[\"attention_mask\"]\n",
        "        \n",
        "        # Convert labels to tensor from numpy arrays\n",
        "        labels_array = np.array(labels)\n",
        "        labels_tensor = torch.from_numpy(labels_array).long()\n",
        "\n",
        "        # Optional: move to GPU here\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels_tensor = labels_tensor.to(device)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels_tensor\n",
        "        }\n",
        "    return collate_fn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Model**\n",
        "\n",
        "\n",
        "This section imports from `model.py` the **LlamaEmotionClassifier**, a transformer-based model designed for **emotion classification**. It utilizes a **fine-tuned Llama model** and a **fully connected classification head** to predict **valence** and **arousal** as discrete categories (low, mid, high).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import the model class definition from the model.py file\n",
        "from model import LlamaEmotionClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Model Architecture**\n",
        "\n",
        "The model consists of the following components:\n",
        "\n",
        "1. **Llama Backbone**  \n",
        "   - A **pre-trained Llama model** serves as the feature extractor.\n",
        "\n",
        "   <div style=\"text-align: left;\">\n",
        "    <img src=\"Images/llama_architecture.png\" width=\"50%\">\n",
        "   </div>\n",
        "\n",
        "   - **LoRA Configuration**  \n",
        "     - $r = 16$  \n",
        "     - $\\text{lora\\_alpha} = 8$  \n",
        "     - $\\text{lora\\_dropout} = 0.1$  \n",
        "     - Applied to the following target modules: $q\\_proj,\\, k\\_proj,\\, v\\_proj,\\, o\\_proj,\\, gate\\_proj,\\, up\\_proj$  \n",
        "\n",
        "   <div style=\"text-align: left;\">\n",
        "    <img src=\"Images/LoRA.png\" width=\"50%\">\n",
        "   </div>\n",
        "\n",
        "   - **Partial Freezing**  \n",
        "     During training, the Llama backbone is **frozen for the first 10 epochs**, then unfrozen thereafter to allow full fine-tuning.\n",
        "\n",
        "\n",
        "2. **Mean + Max Pooling**  \n",
        "   - The final hidden states are pooled in two ways—**mean-pooling** and **max-pooling**—to form a combined representation $Z$.\n",
        "\n",
        "3. **Classification Head**  \n",
        "   - A single **fully-connected (FC) layer** reduces $Z$ to **4 logits**—2 for **valence** and 2 for **arousal**.\n",
        "\n",
        "Mathematically, given an input sequence **$X$**, the model computes:\n",
        "\n",
        "$$\n",
        "H = \\text{Llama}(X)\n",
        "$$\n",
        "\n",
        "where **$H \\in \\mathbb{R}^{B \\times L \\times d}$**, with:\n",
        "- $B$: Batch size,\n",
        "- $L$: Sequence length,\n",
        "- $d$: Hidden state dimension.\n",
        "\n",
        "- **Masked Mean-Pooling**  \n",
        "     $$\n",
        "     \\overline{H} = \\frac{\\sum_{i=1}^{L} H_i \\,\\cdot\\, M_i}{\\sum_{i=1}^{L} M_i},\n",
        "     $$\n",
        "     where $M_i$ is the attention mask (1 for valid tokens, 0 for padding).\n",
        "\n",
        "- **Max-Pooling**  \n",
        "     $$\n",
        "     H^{\\max} = \\max_{1 \\le i \\le L}(\\,H_i \\cdot M_i\\,).\n",
        "     $$\n",
        "\n",
        "- **Concatenation**  \n",
        "     $$\n",
        "     Z = \\bigl[\\;\\overline{H}\\;\\|\\;H^{\\max}\\bigr],\n",
        "     $$\n",
        "     making $Z \\in \\mathbb{R}^{2d}$.\n",
        "\n",
        "The final classification head projects $Z$ from $2d$ to **4 logits**—2 for **valence** and 2 for **arousal**.\n",
        "\n",
        "   $$\n",
        "   y = WZ + b,\n",
        "   $$\n",
        "   where $W \\in \\mathbb{R}^{4 \\times 2d}$ and $b \\in \\mathbb{R}^{4}$.  \n",
        "   - The first 2 logits correspond to **valence** (binary classes).  \n",
        "   - The last 2 logits correspond to **arousal** (binary classes).\n",
        "\n",
        "**Loss Function**\n",
        "\n",
        "The model is trained using **categorical cross-entropy loss**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\mathcal{L}_{val} + \\mathcal{L}_{aro}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{val} = - \\sum_{c=0}^{1} y_c^{val} \\log(\\hat{y}_c^{val})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{aro} = - \\sum_{c=0}^{1} y_c^{aro} \\log(\\hat{y}_c^{aro})\n",
        "$$\n",
        "\n",
        "- **$y_c$** is the true label (one-hot encoded),\n",
        "- **$\\hat{y}_c$** is the predicted probability after softmax over the 2 valence or 2 arousal logits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Erz75bfyR_Ls"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model on device: cuda:3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaEmotionClassifier initialized.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Llama emotion classifier\n",
        "llama_classifier = LlamaEmotionClassifier(model_path=model_path, device=device, train_folder=\"Trainings\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpoakmq_cyVw"
      },
      "source": [
        "# **Training**\n",
        "\n",
        "This section defines the **hyperparameters** for model training, splits the dataset into **train, validation, and test sets**, sets up **data loaders** and trains the `LlamaEmotionClassifier` model. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hparams:\n",
            "epochs: 20\n",
            "batch_size: 32\n",
            "learning_rate: 0.001\n",
            "train_split: 0.67\n",
            "val_split: 0.13\n",
            "max_length: 128\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "hparams = {\n",
        "    \"epochs\": 20,\n",
        "    \"batch_size\": 8,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"train_split\": 0.67,   \n",
        "    \"val_split\": 0.13,\n",
        "    \"max_length\": 128\n",
        "}\n",
        "\n",
        "# Print Hyperparameters for verification\n",
        "print(\"hparams:\")\n",
        "for key, value in hparams.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4LDTZPeYjuJT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset splits => train: 6860, val: 1331, test: 2049\n"
          ]
        }
      ],
      "source": [
        "# Train/val/test split\n",
        "total_len = len(dataset)\n",
        "train_len = int(hparams[\"train_split\"] * total_len)\n",
        "val_len = int(hparams[\"val_split\"] * total_len)\n",
        "test_len = total_len - (train_len + val_len)\n",
        "\n",
        "indices = torch.randperm(total_len).tolist()  # Fixed permutation\n",
        "train_indices = indices[:train_len]\n",
        "val_indices = indices[train_len:train_len+val_len]\n",
        "test_indices = indices[train_len+val_len:]\n",
        "\n",
        "index_dict = {\n",
        "        \"train_indices\": train_indices,\n",
        "        \"val_indices\": val_indices,\n",
        "        \"test_indices\": test_indices\n",
        "}\n",
        "\n",
        "# Create dataset subsets\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "val_dataset = Subset(dataset, val_indices)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "print(f\"Dataset splits => train: {len(train_dataset)}, val: {len(val_dataset)}, test: {len(test_dataset)}\")\n",
        "\n",
        "# DataLoaders\n",
        "\n",
        "collate_fn = dynamic_tokenize_collate_fn(\n",
        "    tokenizer=llama_classifier.tokenizer,\n",
        "    max_length=hparams[\"max_length\"],\n",
        "    device=device,\n",
        "    debug=False\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Training Procedure**\n",
        "\n",
        "The model is fine-tuned using **AdamW optimization** with a **cross-entropy loss** objective. The training loop includes:\n",
        "\n",
        "1. **Batch Processing**  \n",
        "   - Tokenization using **Llama's tokenizer**.\n",
        "   - Padded sequences are masked to **ignore padding in pooling**.\n",
        "\n",
        "2. **Forward Pass**  \n",
        "   - Llama extracts **hidden states** from input sequences.\n",
        "   - The **pooled hidden state** is passed through the classification head.\n",
        "\n",
        "3. **Loss Computation & Backpropagation**  \n",
        "   - The **cross-entropy loss** is computed for valence and arousal.\n",
        "   - Gradients are computed and updated using **AdamW**.\n",
        "\n",
        "4. **Validation & Accuracy Tracking**  \n",
        "   - **Valence and arousal accuracy** are measured separately.\n",
        "   - **Overall accuracy** considers both dimensions jointly.\n",
        "\n",
        "The **overall accuracy** is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}(\\hat{y}_i^{val} = y_i^{val} \\land \\hat{y}_i^{aro} = y_i^{aro})\n",
        "$$\n",
        "\n",
        "where:\n",
        "- **$N$** is the number of samples,\n",
        "- **$\\mathbb{1}$** is an indicator function that returns 1 if both predictions match the true labels.\n",
        "\n",
        "**Model Deployment**\n",
        "\n",
        "After training, the model is saved in the experiment folder, along with:\n",
        "- Model weights and architecture.\n",
        "- Training curves for **loss** and **accuracy**.\n",
        "- A saved notebook for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ycHeWJZPr5f3",
        "outputId": "459c343b-f29a-4be6-db98-7ac8f1ab7f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training with cross-entropy classification...\n",
            "Experiment folder created: Trainings/20250328_201357\n",
            "Hyperparameters saved to: Trainings/20250328_201357/hyperparams.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 [TRAIN]:   0%|          | 0/215 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "                                                                                                                              \r"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 200.88 MiB is free. Including non-PyTorch memory, this process has 47.33 GiB memory in use. Of the allocated memory 43.64 GiB is allocated by PyTorch, and 3.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mllama_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_dict\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/model.py:287\u001b[0m, in \u001b[0;36mLlamaEmotionClassifier.train_model\u001b[0;34m(self, train_loader, val_loader, hparams, index_dict)\u001b[0m\n\u001b[1;32m    284\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    285\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 287\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m loss, val_acc, aro_acc, overall_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_and_accuracy(logits, labels)\n\u001b[1;32m    290\u001b[0m val_running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/model.py:120\u001b[0m, in \u001b[0;36mLlamaEmotionClassifier.forward\u001b[0;34m(self, input_ids, attention_mask, freeze_llm)\u001b[0m\n\u001b[1;32m    118\u001b[0m grad_no_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39menable_grad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m freeze_llm \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m grad_no_grad():\n\u001b[0;32m--> 120\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# outputs = self.model(\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#         input_ids=input_ids,\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m#         attention_mask=attention_mask,\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#         output_hidden_states=True,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#         return_dict=True\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m    133\u001b[0m last_hidden \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/peft/peft_model.py:1719\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1718\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:842\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    857\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:594\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    583\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    584\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m         position_embeddings,\n\u001b[1;32m    592\u001b[0m     )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 594\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:352\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    351\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 352\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    355\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:190\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 190\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:496\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# sure.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mclone()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:484\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    480\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    482\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:533\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/envs/tesi_giorgio/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:462\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    465\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 3 has a total capacity of 47.53 GiB of which 200.88 MiB is free. Including non-PyTorch memory, this process has 47.33 GiB memory in use. Of the allocated memory 43.64 GiB is allocated by PyTorch, and 3.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "llama_classifier.train_model(train_loader, val_loader, hparams, index_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Testing**\n",
        "\n",
        "This section **loads a trained model**, **evaluates it on the test set**, and **saves the results**. The trained model is retrieved from a saved experiment folder, and the **test dataset** is reconstructed to measure **valence, arousal, and overall classification accuracy**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Load the Experiment Setup  \n",
        "- The **experiment folder** containing model checkpoints and configurations is specified.\n",
        "- The Python path is updated to allow importing the model class definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Specify experiment folder\n",
        "experiment_folder = os.path.join(project_path, \"Trainings/20250228_101952\")  # Change this to the correct experiment name\n",
        "\n",
        "sys.path.append(experiment_folder)\n",
        "\n",
        "# Now import the class\n",
        "from model_definition import LlamaEmotionClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Load Dataset Indices & Hyperparameters  \n",
        "- The **test set indices** are retrieved from `dataset_indices.json`, ensuring consistency with previous training/validation splits.\n",
        "- Hyperparameters such as **batch size, sequence length**, and other settings are loaded from `hyperparams.json`.\n",
        "\n",
        "3. Load the Pretrained Model  \n",
        "- The model class (`LlamaEmotionClassifier`) is imported from the saved **model definition** file.\n",
        "- A new instance of the classifier is initialized with the same **model path and device settings**.\n",
        "- Trained **model weights** are loaded from `model_weights.pt`, ensuring the model is in the exact trained state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset indices\n",
        "index_path = os.path.join(experiment_folder, \"dataset_indices.json\")\n",
        "with open(index_path, \"r\") as f:\n",
        "    index_dict = json.load(f)\n",
        "\n",
        "test_indices = index_dict[\"test_indices\"]\n",
        "\n",
        "# Load hyperparameters\n",
        "hparams_path = os.path.join(experiment_folder, \"hyperparams.json\")\n",
        "with open(hparams_path, \"r\") as f:\n",
        "    hparams = json.load(f)\n",
        "\n",
        "print(f\"Hyperparameters loaded from {hparams_path}: {hparams}\")\n",
        "\n",
        "# Load model class definition from saved file\n",
        "model_definition_path = os.path.join(experiment_folder, \"model_definition.py\")\n",
        "\n",
        "print(f\"Model class definition loaded from {model_definition_path}\")\n",
        "\n",
        "# Load trained model\n",
        "model = LlamaEmotionClassifier(model_path=model_path, device=device, train_folder=\"Trainings\")\n",
        "weights_path = os.path.join(experiment_folder, \"model_weights.pt\")\n",
        "model.load_state_dict(torch.load(weights_path), strict=False)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model weights loaded from {weights_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Create the Test Dataset & DataLoader  \n",
        "- The **test subset** is reconstructed using the previously saved test indices.\n",
        "- A **data collate function** (`dynamic_tokenize_collate_fn`) processes test samples into tokenized tensors.\n",
        "- The **test DataLoader** batches the dataset for efficient inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create test dataset\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "# Create test DataLoader\n",
        "\n",
        "# Build the collate_fn\n",
        "collate_fn = dynamic_tokenize_collate_fn(\n",
        "    tokenizer=model.tokenizer,\n",
        "    max_length=hparams[\"max_length\"],\n",
        "    device=device,\n",
        "    debug=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Test dataset loaded: {len(test_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Run Model Testing  \n",
        "- The trained model is evaluated on the test set using **accuracy metrics**:\n",
        "  - **Valence Accuracy**: Measures correct classification of valence levels (low, mid, high).\n",
        "  - **Arousal Accuracy**: Measures correct classification of arousal levels.\n",
        "  - **Overall Accuracy**: Counts instances where both valence and arousal predictions are correct.\n",
        "\n",
        "6. Save Test Results  \n",
        "- Test accuracy metrics are written to `test_results.txt` within the experiment folder.\n",
        "\n",
        "7. Rename the Experiment Folder with Results  \n",
        "- The experiment folder is renamed to include **final test accuracies**, making it easy to track performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the test and get the results\n",
        "test_results = model.test_model(test_loader)\n",
        "\n",
        "# Save the results to test_results.txt inside experiment_folder\n",
        "results_file_path = os.path.join(experiment_folder, \"test_results.txt\")\n",
        "with open(results_file_path, \"w\") as f:\n",
        "    f.write(f\"Valence Accuracy: {test_results['valence_accuracy']:.4f}\\n\")\n",
        "    f.write(f\"Arousal Accuracy: {test_results['arousal_accuracy']:.4f}\\n\")\n",
        "    f.write(f\"Overall Accuracy: {test_results['overall_accuracy']:.4f}\\n\")\n",
        "\n",
        "print(f\"Test results saved to {results_file_path}\")\n",
        "\n",
        "# Format the new folder name with test accuracy values\n",
        "new_experiment_folder = f\"{experiment_folder}_val={test_results['valence_accuracy']:.4f}_aro={test_results['arousal_accuracy']:.4f}_total={test_results['overall_accuracy']:.4f}\"\n",
        "\n",
        "# Rename the folder\n",
        "os.rename(experiment_folder, new_experiment_folder)\n",
        "\n",
        "print(f\"Experiment folder renamed to: {new_experiment_folder}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ffV7pmvlrUE4",
        "REX1ykXjmI8q"
      ],
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tesi_giorgio",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06953c1b91594d07a55647a6f5e899c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17eeadc4e20740d9abe046b3ffb06b0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40941f2564b340ef89f727a0dba7d870": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62bb4df9a5034aeaaab12e1eb0167ef6",
              "IPY_MODEL_e07de58be930499ca231cc4354ce49ae",
              "IPY_MODEL_4494daf5f2854dde9701a7219941f621"
            ],
            "layout": "IPY_MODEL_d21708c45e3648dc9fed6fc39b12197c"
          }
        },
        "4494daf5f2854dde9701a7219941f621": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da3229a834e6406fb5cc2de18c8c60c1",
            "placeholder": "​",
            "style": "IPY_MODEL_fcbcbd09a204459aa442cb902dd7ec3f",
            "value": " 4/4 [05:12&lt;00:00, 64.45s/it]"
          }
        },
        "62bb4df9a5034aeaaab12e1eb0167ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17eeadc4e20740d9abe046b3ffb06b0a",
            "placeholder": "​",
            "style": "IPY_MODEL_cfe0c696512f4d8eaa374bb39776d8c3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cfe0c696512f4d8eaa374bb39776d8c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d21708c45e3648dc9fed6fc39b12197c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da3229a834e6406fb5cc2de18c8c60c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e07de58be930499ca231cc4354ce49ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7f1e63b395343adbdc651402c38f065",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06953c1b91594d07a55647a6f5e899c2",
            "value": 4
          }
        },
        "f7f1e63b395343adbdc651402c38f065": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcbcbd09a204459aa442cb902dd7ec3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
